{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoupled Neural Interfaces\n",
    "\n",
    "An implementation of DNIs in TensorFlow. As in the referenced paper, the feasability of this technique is demonstrated through the use of stochastic layer-wise updates when training a fully connected network on the MNIST classification problem.\n",
    "\n",
    "Reference: [Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/xihuan/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/xihuan/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41389fad42b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/xihuan/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/xihuan/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/xihuan/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/xihuan/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-391c17000279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;31m# Used to display training progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/xihuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/xihuan/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/xihuan/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tqdm import tqdm # Used to display training progress bar\n",
    "\n",
    "sg_sess = tf.Session()\n",
    "backprop_sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "Architecture and training largely follow the details provided in section **A.1 Feed-Forward Implementation Details** of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data scaled to [0,1] interval, and labels in one-hot format\n",
    "# 55k train, 5k validation, 10k test\n",
    "MNIST = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "iterations = 500000\n",
    "batch_size = 250 # modified to evenly divide dataset size\n",
    "\n",
    "init_lr = 3e-5\n",
    "lr_div = 10\n",
    "lr_div_steps = set([300000, 400000])\n",
    "\n",
    "update_prob = 0.2 # Probability of updating a decoupled layer\n",
    "\n",
    "validation_checkpoint = 10 # How often (iterations) to validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for constructing layers\n",
    "def dense_layer(inputs, units, name, output=False):\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.dense(inputs, units, name=\"fc\")\n",
    "        if not output:\n",
    "            x = tf.layers.batch_normalization(x, name=\"bn\")\n",
    "            x = tf.nn.relu(x, name=\"relu\")\n",
    "    return x\n",
    "\n",
    "def sg_module(inputs, units, name, label):\n",
    "    with tf.variable_scope(name):\n",
    "        inputs_c = tf.concat([inputs, label], 1)\n",
    "        x = tf.layers.dense(inputs_c, units, name=\"fc\", kernel_initializer=tf.zeros_initializer())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ops for network architecture\n",
    "with tf.variable_scope(\"architecture\"):\n",
    "    # Inputs\n",
    "    with tf.variable_scope(\"input\"):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "        Y = tf.placeholder(tf.float32, shape=(None, 10), name=\"labels\") # Target\n",
    "    \n",
    "    # Inference layers\n",
    "    h1 = dense_layer(X, 256, \"layer1\")\n",
    "    h2 = dense_layer(h1, 256, name=\"layer2\")\n",
    "    h3 = dense_layer(h2, 256, name=\"layer3\")\n",
    "    logits = dense_layer(h3, 10, name=\"layer4\", output=True)\n",
    "    \n",
    "    # Synthetic Gradient layers\n",
    "    d1_hat = sg_module(h1, 256, \"sg2\", Y)\n",
    "    d2_hat = sg_module(h2, 256, \"sg3\", Y)\n",
    "    d3_hat = sg_module(h3, 256, \"sg4\", Y)\n",
    "\n",
    "# Collections of trainable variables in each block\n",
    "layer_vars = [tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer1/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer2/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer3/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer4/\")]\n",
    "sg_vars = [None,\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg2/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg3/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg4/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for optimizing a layer and its synthetic gradient module\n",
    "def train_layer_n(n, h_m, h_n, d_hat_m, class_loss, d_n=None):\n",
    "    with tf.variable_scope(\"layer\"+str(n)):\n",
    "        layer_grads = tf.gradients(h_n, [h_m]+layer_vars[n-1], d_n)\n",
    "        layer_gv = list(zip(layer_grads[1:],layer_vars[n-1]))\n",
    "        layer_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).apply_gradients(layer_gv)\n",
    "    with tf.variable_scope(\"sg\"+str(n)):\n",
    "        d_m = layer_grads[0]\n",
    "        sg_loss = tf.divide(tf.losses.mean_squared_error(d_hat_m, d_m), class_loss)\n",
    "        sg_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(sg_loss, var_list=sg_vars[n-1])\n",
    "    return layer_opt, sg_opt\n",
    "\n",
    "# Ops for training\n",
    "with tf.variable_scope(\"train\"):\n",
    "    with tf.variable_scope(\"learning_rate\"):\n",
    "        learning_rate = tf.Variable(init_lr, dtype=tf.float32, name=\"lr\")\n",
    "        reduce_lr = tf.assign(learning_rate, learning_rate/lr_div, name=\"lr_decrease\")\n",
    "\n",
    "    pred_loss = tf.losses.softmax_cross_entropy(onehot_labels=Y, logits=logits, scope=\"prediction_loss\")\n",
    "    \n",
    "    # Optimizers when using synthetic gradients\n",
    "    with tf.variable_scope(\"synthetic\"):\n",
    "        layer4_opt, sg4_opt = train_layer_n(4, h3, pred_loss, d3_hat, pred_loss)\n",
    "        layer3_opt, sg3_opt = train_layer_n(3, h2, h3, d2_hat, pred_loss, d3_hat)\n",
    "        layer2_opt, sg2_opt = train_layer_n(2, h1, h2, d1_hat, pred_loss, d2_hat)\n",
    "        with tf.variable_scope(\"layer1\"):\n",
    "            layer1_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(h1, var_list=layer_vars[0], grad_loss=d1_hat)\n",
    "    \n",
    "    # Optimizer when using backprop\n",
    "    with tf.variable_scope(\"backprop\"):\n",
    "        backprop_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(pred_loss)\n",
    "        \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ops for validation and testing (computing classification accuracy)\n",
    "with tf.variable_scope(\"test\"):\n",
    "    preds = tf.nn.softmax(logits, name=\"predictions\")\n",
    "    correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(Y,1), name=\"correct_predictions\")\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32), name=\"correct_prediction_count\") / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ops for tensorboard summary data\n",
    "with tf.variable_scope(\"summary\"):\n",
    "    cost_summary_opt = tf.summary.scalar(\"loss\", pred_loss)\n",
    "    accuracy_summary_opt = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "With training using backprop, each layer cannot be updated until the gradient from the loss is calculated for the following layer. This update-locking is avoided when using synthetic gradients, as the next layer's gradients are approximated by doing a forward pass through that layer's synthetic gradient module. However, synthetic gradient modules themselves are update-locked to the layer they are grouped with, as updating the gradient estimation will require the true gradient which we only calculate when updating the layer itself. Though not used as such here, the benefit of using synthetic gradients is that it allows training of layers to be parallelized across many GPUs, which would be a significant speedup when training large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 500000/500000 [58:37<00:00, 160.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train using backprop as benchmark\n",
    "with backprop_sess.as_default():\n",
    "    backprop_train_writer = tf.summary.FileWriter(\"logging/backprop/train\")\n",
    "    backprop_validation_writer = tf.summary.FileWriter(\"logging/backprop/validation\")\n",
    "\n",
    "    backprop_sess.run(init)\n",
    "    for i in tqdm(range(1,iterations+1)):\n",
    "        if i in lr_div_steps: # Decrease learning rate\n",
    "            backprop_sess.run(reduce_lr)\n",
    "        \n",
    "        data, target = MNIST.train.next_batch(batch_size)\n",
    "        _, summary = backprop_sess.run([backprop_opt, summary_op], feed_dict={X:data,Y:target})\n",
    "        backprop_train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = MNIST.test.next_batch(batch_size)\n",
    "            summary = backprop_sess.run([summary_op], feed_dict={X:Xb,Y:Yb})[0]\n",
    "            backprop_validation_writer.add_summary(summary, i)\n",
    "\n",
    "    # Cleanup summary writers\n",
    "    backprop_train_writer.close()\n",
    "    backprop_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 500000/500000 [37:46<00:00, 220.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train using synthetic gradients\n",
    "with sg_sess.as_default():\n",
    "    sg_train_writer = tf.summary.FileWriter(\"logging/sg/train\", sg_sess.graph)\n",
    "    sg_validation_writer = tf.summary.FileWriter(\"logging/sg/validation\")\n",
    "    \n",
    "    sg_sess.run(init)\n",
    "    for i in tqdm(range(1,iterations+1)):\n",
    "        if i in lr_div_steps: # Decrease learning rate\n",
    "            sg_sess.run(reduce_lr)\n",
    "        \n",
    "        data, target = MNIST.train.next_batch(batch_size)\n",
    "        \n",
    "        # Each layer can now be independently updated (could be parallelized)\n",
    "        if random.random() <= update_prob: # Stochastic updates are possible\n",
    "            sg_sess.run([layer1_opt], feed_dict={X:data,Y:target})\n",
    "        if random.random() <= update_prob:\n",
    "            sg_sess.run([layer2_opt, sg2_opt], feed_dict={X:data,Y:target})\n",
    "        if random.random() <= update_prob:\n",
    "            sg_sess.run([layer3_opt, sg3_opt], feed_dict={X:data,Y:target})\n",
    "        if random.random() <= update_prob:\n",
    "            _, _, summary = sg_sess.run([layer4_opt, sg4_opt, summary_op], feed_dict={X:data,Y:target})\n",
    "            sg_train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = MNIST.test.next_batch(batch_size)\n",
    "            summary = sg_sess.run([summary_op], feed_dict={X:Xb,Y:Yb})[0]\n",
    "            sg_validation_writer.add_summary(summary, i)\n",
    "    \n",
    "    # Cleanup summary writers\n",
    "    sg_train_writer.close()\n",
    "    sg_validation_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphed on TensorBoard, we see that the stochastically updated network manages to reach the validation accuracy of the network trained using backprop:  \n",
    "\n",
    "Orange - backprop/train  \n",
    "Light Blue - backprop/validate  \n",
    "Purple - sg/train  \n",
    "Dark Blue - sg/validate\n",
    "\n",
    "![Accuracy](images/dni_accuracy.png) ![Loss](images/dni_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.351406607963\n",
      "0.976400002837\n"
     ]
    }
   ],
   "source": [
    "# Test using backprop\n",
    "with backprop_sess.as_default():\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "    test_accuracy = 0\n",
    "    test_loss = 0\n",
    "    for _ in range(n_batches):\n",
    "        Xb, Yb = MNIST.test.next_batch(batch_size)\n",
    "        batch_accuracy, batch_loss = backprop_sess.run([accuracy, pred_loss], feed_dict={X:Xb,Y:Yb})\n",
    "        test_loss += batch_loss\n",
    "        test_accuracy += batch_accuracy\n",
    "    print (test_loss/n_batches)\n",
    "    print(test_accuracy/n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0989040184766\n",
      "0.969500003755\n"
     ]
    }
   ],
   "source": [
    "# Test using synthetic gradients\n",
    "with sg_sess.as_default():\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "    test_accuracy = 0\n",
    "    test_loss = 0\n",
    "    for _ in range(n_batches):\n",
    "        Xb, Yb = MNIST.test.next_batch(batch_size)\n",
    "        batch_accuracy, batch_loss = sg_sess.run([accuracy, pred_loss], feed_dict={X:Xb,Y:Yb})\n",
    "        test_loss += batch_loss\n",
    "        test_accuracy += batch_accuracy\n",
    "    print (test_loss/n_batches)\n",
    "    print(test_accuracy/n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "sg_sess.close()\n",
    "backprop_sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
